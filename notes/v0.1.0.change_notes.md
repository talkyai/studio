# TalkyAI Studio — Release Notes (v0.1.0)

**Release Date:** August 13, 2025

This version marks the first public release of TalkyAI Studio under its new name. The application combines cloud AI APIs with local LLM servers, providing powerful tools for quick configuration switching, model management, and context handling.

— Supports Windows, macOS, and Linux  
— All data and keys stored locally

## Key New Features and Changes

### 1) Rebranding
- Project renamed to "TalkyAI Studio"
- Updated window title, productName, and bundle identifier

### 2) Projects: New Hierarchy and UX
- Project manager now displays projects at the top level
- Each project shows its configurations (model, mode, provider, server)
- Quick project activation with associated prompt history (when available)
- Fully internationalized (RU/EN)

### 3) SystemMonitor Widget
- Compact RAM/CPU monitor in AppBar
- Updates every 2 seconds with localized tooltip
- Tauri backend command returns current metrics (CPU%, RAM used/total)

### 4) Ollama: Advanced /api/chat Parameters
- Added "Ollama Extra Params (JSON)" field in settings
- Parameters (stream, tools, format/schema, keep_alive, think, options: seed/stop etc.) are deep-merged into /api/chat request body
- Invalid JSON is gracefully ignored

### 5) Cross-Platform Server Installation
- Download and install llama.cpp/Ollama binaries for Windows, macOS, and Linux
- Supports: .zip (Win/macOS), .tgz (Linux), .dmg (macOS Ollama installer)
- Progress tracking: Download → Extract → Ready

### 6) Target OS Selection for Installation
- Manual OS selection (Windows/macOS/Linux) in "Server Installation" section
- Backend downloads appropriate artifacts to runtime/{server}/{variant}/

### 7) i18n and Rust Text Standardization
- Added translations for ProjectsMenu and SystemMonitor with fallback labels
- Standardized all .rs file texts to English (diagnostics/logs/error messages)

### 8) Documentation
- Updated README.md (RU) and added README_EN.md (EN)
- Removed tech stack details, focusing on features, use cases, and privacy
- Added dedicated README_RU.md (extended Russian documentation)

### 9) Builds and Distribution
- GitHub Actions: Cross-platform workflow for Windows/macOS/Linux builds
- Linux: Base configurations for DEB/RPM (WebKitGTK/GTK deps, desktop file, install scripts)
- Dockerfile.linux for local Linux package builds

## Supported Providers and Modes (v0.1.0)
- **Cloud APIs**:
    - DeepSeek API (default endpoint configured)
    - OpenAI-compatible services (custom Base URL)
- **Local Servers**:
    - llama.cpp (CPU/ARM/CUDA/HIP/Vulkan variants)
    - Ollama (local models, auto-pull on startup)

## Model Management
- **Ollama**:
    - Local model listing with refresh
    - Model pull progress in UI
    - Advanced chat params via JSON
- **llama.cpp**:
    - GGUF repository and filename/port configuration
    - Generation params: temperature, top_k, top_p, min_p, max_tokens, repeatLastN

## UI/UX Improvements
- Projects - Faster navigation with clear structure
- Dedicated system monitoring widget in header
- Theme (light/dark) and language (RU/EN) switching with local persistence

## Developer Changes
- Backend: Unified Tauri commands for server downloads (OS/arch-aware), binary checks, and system monitoring
- Centralized artifact download helpers for llama.cpp/Ollama
- Deep JSON parameter merging for Ollama requests

## Known Limitations
- GPU/VRAM metrics not yet cross-platform (shows as N/A - future NVML/ROCm SMI/Metal support planned)
- macOS Ollama DMG requires manual installation (double-click)
- For broad Linux compatibility, build on older systems (glibc) or use GitHub Actions/containers

## Potential Breaking Changes
- Product rename changes bundle/desktop identifiers. Existing shortcuts may need updating. User data remains intact.

## Upgrade Recommendations
1. If using previous version:
    - Reinstall to update shortcuts/identifiers
2. For local servers:
    - Redownload binaries via "Server Installation" if needed
3. Ollama users:
    - Verify base URL (default: http://127.0.0.1:11434) and model name
    - Validate JSON if using advanced parameters

## Roadmap Preview
- Chat history with search and export (Markdown/PDF/JSON)
- GPU/VRAM monitoring in SystemMonitor
- New backends (vLLM, TGI), EXL2 support
- Provider plugins and REST API for integrations
- Security: Optional local data encryption, "incognito" mode

## Acknowledgments
Thank you to all beta testers, contributors, and bug reporters.

— The TalkyAI Studio Team